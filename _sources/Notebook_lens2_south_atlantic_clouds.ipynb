{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CESM2 - LARGE ENSEMBLE (LENS2)\n",
    "\n",
    "- This notebook aims to compute the average of essential terms for incident solar radiation in the South Atlantic, such as cloud cover fraction and thickness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import dask\n",
    "import cf_xarray\n",
    "import intake\n",
    "import cftime\n",
    "import nc_time_axis\n",
    "import intake_esm\n",
    "import matplotlib.pyplot as plt\n",
    "import pop_tools\n",
    "from dask.distributed import Client, wait\n",
    "from ncar_jobqueue import NCARCluster\n",
    "import warnings, getpass, os\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import cartopy.crs as ccrs\n",
    "import cmocean\n",
    "import dask\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_per_worker = 20 # memory per worker in GB \n",
    "num_workers = 40 # number of workers\n",
    "cluster = NCARCluster(cores=1,\n",
    "                      processes=1,\n",
    "                      memory=f'{mem_per_worker} GB',\n",
    "                      resource_spec=f'select=1:ncpus=1:mem={mem_per_worker}GB',\n",
    "                      walltime='1:00:00',\n",
    "                      log_directory='./dask-logs',\n",
    "                     )\n",
    "cluster.scale(num_workers)\n",
    "client = Client(cluster)\n",
    "print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_esm_datastore(\n",
    "    '/glade/collections/cmip/catalog/intake-esm-datastore/catalogs/glade-cesm2-le.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ocean Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_vars = ['TGCLDLWP','FSDS','FSNS','CLDTOT','SST','OCNFRAC']\n",
    "cat_subset = catalog.search(component='atm',variable=all_vars,frequency='month_1')\n",
    "# Load catalog entries for subset into a dictionary of xarray datasets\n",
    "dset_dict_raw  = cat_subset.to_dataset_dict(zarr_kwargs={'consolidated': True}, storage_options={'anon': True})#, xarray_open_kwargs=('chunks': {'':}))\n",
    "print(f'\\nDataset dictionary keys:\\n {dset_dict_raw.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Concatenation of variables\n",
    "ff=('cmip6','smbb')                      # Forcings\n",
    "ds_dict = dict()\n",
    "for var in all_vars:\n",
    "    # 1- combine historical and ssp370 (concatenate in time)\n",
    "    ds_dict_tmp = dict()\n",
    "    for scenario in ff:\n",
    "        ds_dict_tmp[scenario] = xr.combine_nested([dset_dict_raw[f'atm.historical.cam.h0.{scenario}.{var}'], dset_dict_raw[f'atm.ssp370.cam.h0.{scenario}.{var}']],concat_dim=['time'])\n",
    "        \n",
    "        # 2- combine cmip6 and smbb (concatenate in member_id)\n",
    "    ds_dict[var] = xr.combine_nested([ds_dict_tmp['cmip6'], ds_dict_tmp['smbb']], concat_dim=['member_id'])\n",
    "    del ds_dict_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask \n",
    "- We need to mask the data of the atmospheric component over the continent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Replace the SST data equal to 0 (continents) by NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['SST']['SST'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['SST']=ds_dict['SST'].where(ds_dict['SST'] != 0.)\n",
    "ds_dict['SST']['SST'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Building the mask. The ocean model and the atmospheric model feed data into the coastal region, so we need to take data that is 100% on the ocean model grid to ensure that we are not looking at data over the continent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['OCNFRAC']['OCNFRAC'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2 Since we are not working with polar regions, we will put NAN on all data that is different from 1 to build the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['OCNFRAC']=ds_dict['OCNFRAC'].where(ds_dict['OCNFRAC'] ==1.)\n",
    "ds_dict['OCNFRAC']['OCNFRAC'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ocean = 2 * np.ones((ds_dict['OCNFRAC'].dims['lat'], ds_dict['OCNFRAC'].dims['lon'])) * np.isfinite(ds_dict['OCNFRAC'].OCNFRAC.isel(time=0,member_id=0))  \n",
    "mask_land = 1 * np.ones((ds_dict['OCNFRAC'].dims['lat'], ds_dict['OCNFRAC'].dims['lon'])) * np.isnan(ds_dict['OCNFRAC'].OCNFRAC.isel(time=0,member_id=0))  \n",
    "mask_array = mask_ocean + mask_land\n",
    "mask_array.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Applying the mask for the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['TGCLDLWP'] =ds_dict['TGCLDLWP'].where(mask_array == 2.)  \n",
    "ds_dict['FSDS'] =ds_dict['FSDS'].where(mask_array == 2.) \n",
    "ds_dict['CLDTOT'] =ds_dict['CLDTOT'].where(mask_array == 2.)  \n",
    "ds_dict['FSNS'] =ds_dict['FSNS'].where(mask_array == 2.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['FSDS']['FSDS'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['FSNS']['FSNS'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['CLDTOT']['CLDTOT'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['TGCLDLWP']['TGCLDLWP'].isel(member_id=0,time=0).plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut and center the variable in the South Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cutting out and centering the variables in the South Atlantic\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "# Ocean component\n",
    "ilon1, flon1, ilon2, flon2 = 245, 288, 0, 17 # longitude (initial (i), final (f)) \n",
    "ilan=0\n",
    "ilas=-34\n",
    "fb=['TGCLDLWP','FSDS','FSNS','CLDTOT']\n",
    "for var in fb:\n",
    "    if var not in ds_dict:\n",
    "        continue\n",
    "    ds_dict[f'{var}']=xr.combine_nested([[\n",
    "        ds_dict[f'{var}'].where((ds_dict[f'{var}'].lat >= ilas) & (ds_dict[f'{var}'].lat <= ilan), drop=True).isel(\n",
    "            lon = slice(ilon1,flon1)),\n",
    "        ds_dict[f'{var}'].where((ds_dict[f'{var}'].lat >= ilas) & (ds_dict[f'{var}'].lat <= ilan), drop=True).isel(\n",
    "            lon = slice(ilon2,flon2))]],\n",
    "        concat_dim=['lat','lon'])\n",
    "    ds_dict[f'{var}'].coords['lon'] = (ds_dict[f'{var}'].coords['lon'] + 180) % 360 - 180 \n",
    "    ds_dict[f'{var}'] = ds_dict[f'{var}'].sortby(ds_dict[f'{var}'].lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['FSDS']['FSDS'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['TGCLDLWP']['TGCLDLWP'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['FSNS']['FSNS'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict['CLDTOT']['CLDTOT'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_CLDTOT=ds_dict['CLDTOT']['CLDTOT'].mean(dim=['member_id','lon','lat']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31'))\n",
    "ds_FSNS=ds_dict['FSNS']['FSNS'].mean(dim=['member_id','lon','lat']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31'))\n",
    "ds_TGCLDLWP=ds_dict['TGCLDLWP']['TGCLDLWP'].mean(dim=['member_id','lon','lat']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31'))\n",
    "ds_FSDS=ds_dict['FSDS']['FSDS'].mean(dim=['member_id','lon','lat']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ticks(ax, ticks, round_to=0, center=True):\n",
    "    upperbound = np.ceil(ax.get_ybound()[1]/round_to)\n",
    "    lowerbound = upperbound-0.84\n",
    "    #lowerbound = np.floor(ax.get_ybound()[0]/round_to)\n",
    "    dy = upperbound - lowerbound\n",
    "    fit = np.floor(dy/(ticks - 1)) + 1\n",
    "    dy_new = (ticks - 1)*fit\n",
    "    if center:\n",
    "        offset = np.floor((dy_new - dy)/2)\n",
    "        lowerbound = lowerbound - offset\n",
    "    values = np.linspace(lowerbound, lowerbound + dy_new, ticks)\n",
    "    return values*round_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x):\n",
    "    return slope * x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLDTOT_sts=ds_CLDTOT.sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(CLDTOT_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, CLDTOT_sts)\n",
    "mymodel_CLDTOT_sts = list(map(myfunc, x))\n",
    "mymodel_CLDTOT_sts=mymodel_CLDTOT_sts\n",
    "m_CLDTOT_sts=slope*10 # per decade\n",
    "p_CLDTOT_sts=p\n",
    "r_CLDTOT_sts=r*r\n",
    "\n",
    "FSNS_sts=ds_FSNS.sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(FSNS_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, FSNS_sts)\n",
    "mymodel_FSNS_sts = list(map(myfunc, x))\n",
    "mymodel_FSNS_sts=mymodel_FSNS_sts\n",
    "m_FSNS_sts=slope*10 # per decade\n",
    "p_FSNS_sts=p\n",
    "r_FSNS_sts=r*r\n",
    "             \n",
    "TGCLDLWP_sts=ds_TGCLDLWP.sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(TGCLDLWP_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, TGCLDLWP_sts)\n",
    "mymodel_TGCLDLWP_sts = list(map(myfunc, x))\n",
    "mymodel_TGCLDLWP_sts=mymodel_TGCLDLWP_sts\n",
    "m_TGCLDLWP_sts=slope*10 # per decade\n",
    "p_TGCLDLWP_sts=p\n",
    "r_TGCLDLWP_sts=r*r\n",
    "\n",
    "FSDS_sts=ds_FSDS.sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(FSDS_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, FSDS_sts)\n",
    "mymodel_FSDS_sts = list(map(myfunc, x))\n",
    "mymodel_FSDS_sts=mymodel_FSDS_sts\n",
    "m_FSDS_sts=slope*10 # per decade\n",
    "p_FSDS_sts=p\n",
    "r_FSDS_sts=r*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_FSNS_sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letts=['A','B','C','D']\n",
    "fig, axs = plt.subplots(1,4, figsize=(25, 7))\n",
    "ds_CLDTOT.plot(ax=axs[0],label=None, linewidth=1,color='blue')\n",
    "ds_FSNS.plot(ax=axs[1],label=None, linewidth=1,color='red')\n",
    "ds_TGCLDLWP.plot(ax=axs[2],label=None, linewidth=1,color='maroon')\n",
    "ds_FSDS.plot(ax=axs[3],label=None, linewidth=1,color='green')\n",
    "axs[0].plot(ds_CLDTOT.sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_CLDTOT_sts,color='blue',linestyle='dashed')\n",
    "axs[0].annotate(f'{m_CLDTOT_sts:.4f} Fraction per decade', xy=(0.009, 0.95), color='blue',fontsize=20,xycoords=axs[0].transAxes)\n",
    "axs[1].plot(ds_FSNS.sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_FSNS_sts,color='red',linestyle='dashed')\n",
    "axs[1].annotate(f'{m_FSNS_sts:.4f} W/m2 per decade', xy=(0.1, 0.95), color='red',fontsize=20,xycoords=axs[1].transAxes)\n",
    "axs[2].plot(ds_TGCLDLWP.sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_TGCLDLWP_sts,color='maroon',linestyle='dashed')\n",
    "axs[2].annotate(f'{m_TGCLDLWP_sts:.4f} kg/m2 per decade', xy=(0.06, 0.95), color='maroon',fontsize=20,xycoords=axs[2].transAxes)\n",
    "axs[3].plot(ds_FSDS.sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_FSDS_sts,color='green',linestyle='dashed')\n",
    "axs[3].annotate(f'{m_FSDS_sts:.4f} W/m2 per decade', xy=(0.1, 0.95), color='green',fontsize=20,xycoords=axs[3].transAxes)\n",
    "for i in range(len(axs)):\n",
    "    axs[i].grid(color='gray', linestyle='-', linewidth=0.7)\n",
    "    axs[i].set_xlabel('Time [Years]',fontsize=16) \n",
    "    axs[i].tick_params(axis='x', labelsize=16); axs[i].tick_params(axis='y', labelsize=16)\n",
    "    at = AnchoredText(letts[i], prop=dict(size=20), frameon=True, loc='lower left'); at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    axs[i].add_artist(at)\n",
    "#    axs[i].set_yticks(calculate_ticks(axs[i], 8))\n",
    "    \n",
    "axs[0].set_ylabel('CLDTOT [Fraction]',fontsize=16)\n",
    "axs[1].set_ylabel('FSNS [W/m2]',fontsize=16)\n",
    "axs[2].set_ylabel('TGCLDLWP [kg/m2]',fontsize=16)\n",
    "axs[3].set_ylabel('FSDS [W/m2]',fontsize=16)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.savefig('clauds.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2022b",
   "language": "python",
   "name": "npl-2022b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
