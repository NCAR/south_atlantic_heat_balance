{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CESM2 - LARGE ENSEMBLE (LENS2)\n",
    "\n",
    "- This notebook aims to compute heat balance terms on the South Atlantic surface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import dask\n",
    "import cf_xarray\n",
    "import intake\n",
    "import cftime\n",
    "import nc_time_axis\n",
    "import intake_esm\n",
    "import matplotlib.pyplot as plt\n",
    "import pop_tools\n",
    "from dask.distributed import Client, wait\n",
    "from ncar_jobqueue import NCARCluster\n",
    "import warnings, getpass, os\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import cartopy.crs as ccrs\n",
    "import cmocean\n",
    "import dask\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_per_worker = 40 # memory per worker in GB \n",
    "num_workers = 40 # number of workers\n",
    "cluster = NCARCluster(cores=1,\n",
    "                      processes=1,\n",
    "                      memory=f'{mem_per_worker} GB',\n",
    "                      resource_spec=f'select=1:ncpus=1:mem={mem_per_worker}GB',\n",
    "                      walltime='2:00:00',\n",
    "                      log_directory='./dask-logs',\n",
    "                     )\n",
    "cluster.scale(num_workers)\n",
    "client = Client(cluster)\n",
    "print(client)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_esm_datastore(\n",
    "    '/glade/collections/cmip/catalog/intake-esm-datastore/catalogs/glade-cesm2-le.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ocean Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_vars = ['LWDN_F','SHF','SHF_QSW','LWUP_F','EVAP_F','SENH_F']\n",
    "cat_subset = catalog.search(component='ocn',variable=all_vars,frequency='month_1')\n",
    "# Load catalog entries for subset into a dictionary of xarray datasets\n",
    "dset_dict_raw  = cat_subset.to_dataset_dict(zarr_kwargs={'consolidated': True}, storage_options={'anon': True})#, xarray_open_kwargs=('chunks': {'':}))\n",
    "print(f'\\nDataset dictionary keys:\\n {dset_dict_raw.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Concatenation of variables\n",
    "ff=('cmip6','smbb')                      # Forcings\n",
    "ds_dict_ocn = dict()\n",
    "for var in all_vars:\n",
    "    # 1- combine historical and ssp370 (concatenate in time)\n",
    "    ds_dict_tmp = dict()\n",
    "    for scenario in ff:\n",
    "        ds_dict_tmp[scenario] = xr.combine_nested([dset_dict_raw[f'ocn.historical.pop.h.{scenario}.{var}'], dset_dict_raw[f'ocn.ssp370.pop.h.{scenario}.{var}']],concat_dim=['time'])\n",
    "        \n",
    "        # 2- combine cmip6 and smbb (concatenate in member_id)\n",
    "    ds_dict_ocn[var] = xr.combine_nested([ds_dict_tmp['cmip6'], ds_dict_tmp['smbb']], concat_dim=['member_id'])\n",
    "    del ds_dict_tmp\n",
    "\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor'] = ds_dict_ocn['EVAP_F']['latent_heat_vapor'].chunk(member_id=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Atmosphere Component"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cat_subset = catalog.search(component='atm',variable=['SHFLX','LHFLX','OCNFRAC','SST'],frequency='month_1')\n",
    "# Load catalog entries for subset into a dictionary of xarray datasets\n",
    "dset_dict_raw  = cat_subset.to_dataset_dict(zarr_kwargs={'consolidated': True}, storage_options={'anon': True})\n",
    "print(f'\\nDataset dictionary keys:\\n {dset_dict_raw.keys()}')\n",
    "# Concatenation of variables\n",
    "ff=('cmip6','smbb')                      # Forcings\n",
    "fb=(['SHFLX','LHFLX','OCNFRAC','SST']) # Variable\n",
    "ds_dict_atm = dict()\n",
    "for var in fb:\n",
    "    # 1- combine historical and ssp370 (concatenate in time)\n",
    "    ds_dict_tmp = dict()\n",
    "    for scenario in ff:\n",
    "        ds_dict_tmp[scenario] = xr.combine_nested([dset_dict_raw[f'atm.historical.cam.h0.{scenario}.{var}'], dset_dict_raw[f'atm.ssp370.cam.h0.{scenario}.{var}']],concat_dim=['time'])\n",
    "        \n",
    "        # 2- combine cmip6 and smbb (concatenate in member_id)\n",
    "    ds_dict_atm[var] = xr.combine_nested([ds_dict_tmp['cmip6'], ds_dict_tmp['smbb']], concat_dim=['member_id'])\n",
    "    del ds_dict_tmp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = '/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/month_1/AREA/b.e21.BSSP370smbb.f09_g17.LE2-1301.020.cam.h0.AREA.201501-202412.nc'\n",
    "atm_AREA = xr.open_mfdataset(path,parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import POP grid for the ocean component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_grid = pop_tools.get_grid('POP_gx1v7')\n",
    "ds_dict_ocn['TLONG'] = pop_grid.TLONG; ds_dict_ocn['TLAT'] = pop_grid.TLAT\n",
    "ds_dict_ocn['ULONG'] = pop_grid.ULONG; ds_dict_ocn['ULAT'] = pop_grid.ULAT\n",
    "del pop_grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Mask \n",
    "- We need to mask the data of the atmospheric component over the continent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### 1. Replace the SST data equal to 0 (continents) by NaN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['SST']['SST'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['SST']=ds_dict_atm['SST'].where(ds_dict_atm['SST'] != 0.)\n",
    "ds_dict_atm['SST']['SST'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### 2. Building the mask. The ocean model and the atmospheric model feed data into the coastal region, so we need to take data that is 100% on the ocean model grid to ensure that we are not looking at data over the continent. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['OCNFRAC']['OCNFRAC'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### 2.2 Since we are not working with polar regions, we will put NAN on all data that is different from 1 to build the mask"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['OCNFRAC']=ds_dict_atm['OCNFRAC'].where(ds_dict_atm['OCNFRAC'] ==1.)\n",
    "ds_dict_atm['OCNFRAC']['OCNFRAC'].isel(time=0,member_id=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask_ocean = 2 * np.ones((ds_dict_atm['OCNFRAC'].dims['lat'], ds_dict_atm['OCNFRAC'].dims['lon'])) * np.isfinite(ds_dict_atm['OCNFRAC'].OCNFRAC.isel(time=0,member_id=0))  \n",
    "mask_land = 1 * np.ones((ds_dict_atm['OCNFRAC'].dims['lat'], ds_dict_atm['OCNFRAC'].dims['lon'])) * np.isnan(ds_dict_atm['OCNFRAC'].OCNFRAC.isel(time=0,member_id=0))  \n",
    "mask_array = mask_ocean + mask_land\n",
    "mask_array.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### 3. Applying the mask for the other variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['SHFLX'] =ds_dict_atm['SHFLX'].where(mask_array == 2.)  \n",
    "ds_dict_atm['LHFLX'] =ds_dict_atm['LHFLX'].where(mask_array == 2.) \n",
    "atm_AREA=atm_AREA.where(mask_array == 2.) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['LHFLX']['LHFLX'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['SHFLX']['SHFLX'].isel(member_id=0,time=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "atm_AREA['AREA'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut and center the variable in the South Atlantic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Cutting out and centering the variables in the South Atlantic\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "# Ocean component\n",
    "ilon1, flon1, ilon2, flon2 = 307, 320, 0, 54 # longitude (initial (i), final (f)) \n",
    "ilan=0\n",
    "ilas=-34\n",
    "\n",
    "var = 'EVAP_F'\n",
    "var2 = 'latent_heat_vapor'\n",
    "ds_tmp1 = ds_dict_ocn[var].where((ds_dict_ocn[var].TLAT >= ilas) & (ds_dict_ocn[var].TLAT <= ilan), drop=True).isel(\n",
    "            nlon = slice(ilon1,flon1))\n",
    "ds_tmp2 = ds_dict_ocn[var].where((ds_dict_ocn[var].TLAT >= ilas) & (ds_dict_ocn[var].TLAT <= ilan), drop=True).isel(\n",
    "            nlon = slice(ilon2,flon2))\n",
    "ds_tmp2[var2]\n",
    "# ds_tmp3 = xr.combine_nested([[ds_tmp1, ds_tmp2]], concat_dim=['nlat','nlon'])\n",
    "# ds_tmp4 = ds_tmp3.assign_coords({'nlon': (ds_tmp3[f'{var}'].coords['nlon'] + 180) % 360 - 180})\n",
    "# ds_tmp5 = ds_tmp4.sortby(ds_tmp4.coords['nlon'])\n",
    "# ds_tmp5[var2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cutting out and centering the variables in the South Atlantic\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "# Ocean component\n",
    "ilon1, flon1, ilon2, flon2 = 307, 320, 0, 54 # longitude (initial (i), final (f)) \n",
    "ilan=0\n",
    "ilas=-34\n",
    "fb=(['LWDN_F','SHF','SHF_QSW','LWUP_F','EVAP_F','SENH_F'])\n",
    "for var in fb:\n",
    "    if var not in ds_dict_ocn:\n",
    "        continue\n",
    "    ds_dict_ocn[f'{var}']=xr.combine_nested([[\n",
    "        ds_dict_ocn[f'{var}'].where((ds_dict_ocn[f'{var}'].TLAT >= ilas) & (ds_dict_ocn[f'{var}'].TLAT <= ilan), drop=True).isel(\n",
    "            nlon = slice(ilon1,flon1)),\n",
    "        ds_dict_ocn[f'{var}'].where((ds_dict_ocn[f'{var}'].TLAT >= ilas) & (ds_dict_ocn[f'{var}'].TLAT <= ilan), drop=True).isel(\n",
    "            nlon = slice(ilon2,flon2))]],\n",
    "        concat_dim=['nlat','nlon'])\n",
    "    ds_dict_ocn[f'{var}'].coords['nlon'] = (ds_dict_ocn[f'{var}'].coords['nlon'] + 180) % 360 - 180 \n",
    "    ds_dict_ocn[f'{var}'] = ds_dict_ocn[f'{var}'].sortby(ds_dict_ocn[f'{var}'].nlon)\n",
    "    # ds_tmp1 = ds_dict_ocn[var].where((ds_dict_ocn[var].TLAT >= ilas) & (ds_dict_ocn[var].TLAT <= ilan), drop=True).isel(\n",
    "    #             nlon = slice(ilon1,flon1))\n",
    "    # ds_tmp2 = ds_dict_ocn[var].where((ds_dict_ocn[var].TLAT >= ilas) & (ds_dict_ocn[var].TLAT <= ilan), drop=True).isel(\n",
    "    #             nlon = slice(ilon2,flon2))\n",
    "    # ds_tmp3 = xr.combine_nested([[ds_tmp1, ds_tmp2]], concat_dim=['nlat','nlon'])\n",
    "    # ds_tmp4 = ds_tmp3.assign_coords({'nlon': (ds_tmp3[f'{var}'].coords['nlon'] + 180) % 360 - 180})\n",
    "    # ds_dict_ocn[var] = ds_tmp4.sortby(ds_tmp4.coords['nlon'])\n",
    "    # del ds_tmp1, ds_tmp2, ds_tmp3, ds_tmp4\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var2 = 'latent_heat_vapor'\n",
    "ds_tmp2[var2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor'] = ds_dict_ocn['EVAP_F']['latent_heat_vapor'].persist()\n",
    "wait(ds_dict_ocn['EVAP_F']['latent_heat_vapor'])\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Atmosphere component\n",
    "ilon1, flon1, ilon2, flon2 = 245, 288, 0, 17 # longitude (initial (i), final (f)) \n",
    "ilan=0\n",
    "ilas=-34\n",
    "fb=(['SHFLX','LHFLX'])\n",
    "for var in fb:\n",
    "    ds_dict_atm[f'{var}']=xr.combine_nested([[\n",
    "        ds_dict_atm[f'{var}'].where((ds_dict_atm[f'{var}'].lat >= ilas) & (ds_dict_atm[f'{var}'].lat <= ilan), drop=True).isel(\n",
    "            lon = slice(ilon1,flon1)),\n",
    "        ds_dict_atm[f'{var}'].where((ds_dict_atm[f'{var}'].lat >= ilas) & (ds_dict_atm[f'{var}'].lat <= ilan), drop=True).isel(\n",
    "            lon = slice(ilon2,flon2))]],\n",
    "        concat_dim=['lat','lon'])   \n",
    "    ds_dict_atm[f'{var}'].coords['lon'] = (ds_dict_atm[f'{var}'].coords['lon'] + 180) % 360 - 180 \n",
    "    ds_dict_atm[f'{var}'] = ds_dict_atm[f'{var}'].sortby(ds_dict_atm[f'{var}'].lon)\n",
    "    \n",
    "del ilan, ilas, ilon1, flon1, ilon2, flon2 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Atmosphere component\n",
    "ilon1, flon1, ilon2, flon2 = 245, 288, 0, 17 # longitude (initial (i), final (f)) \n",
    "ilan=0\n",
    "ilas=-34\n",
    "teste=xr.combine_nested([[\n",
    "    atm_AREA['AREA'].where((atm_AREA['AREA'].lat >= ilas) & (atm_AREA['AREA'].lat <= ilan), drop=True).isel(\n",
    "        lon = slice(ilon1,flon1)),\n",
    "    atm_AREA['AREA'].where((atm_AREA['AREA'].lat >= ilas) & (atm_AREA['AREA'].lat <= ilan), drop=True).isel(\n",
    "        lon = slice(ilon2,flon2))]],\n",
    "    concat_dim=['lat','lon'])   \n",
    "teste.coords['lon'] = (teste.coords['lon'] + 180) % 360 - 180 \n",
    "teste = teste.sortby(teste.lon)\n",
    "atm_AREA = teste.isel(time=0)  \n",
    "del ilan, ilas, ilon1, flon1, ilon2, flon2, teste "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Check the variables out"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "atm_AREA.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['LHFLX']['LHFLX'].mean(dim=['member_id','time']).plot()\n",
    "plt.savefig('LH.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_atm['SHFLX']['SHFLX'].mean(dim=['member_id','time']).plot()\n",
    "plt.savefig('SH.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['LWDN_F']['LWDN_F'].mean(dim=['member_id','time']).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['LWUP_F']['LWUP_F'].mean(dim=['member_id','time']).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['SHF']['SHF'].mean(dim=['member_id','time']).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['SHF_QSW']['SHF_QSW'].mean(dim=['member_id','time']).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['SENH_F']['SENH_F'].mean(dim=['member_id','time']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask the continent \n",
    "- The area saved by the ocean component needs to be masked over the continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fb=(['SHF'])\n",
    "warnings.simplefilter(\"ignore\")\n",
    "for var in fb:\n",
    "    mask_array = dict()\n",
    "    mask_ocean = 2 * np.ones((len(ds_dict_ocn[f'{var}'][f'{var}'].coords['nlat']), # ocean\n",
    "                          len(ds_dict_ocn[f'{var}'][f'{var}'].coords['nlon']))\n",
    "                        ) * np.isfinite(ds_dict_ocn[f'{var}'][f'{var}'].isel(time=0))  \n",
    "    mask_land  = 1 * np.ones((len(ds_dict_ocn[f'{var}'][f'{var}'].coords['nlat']), # continent\n",
    "                          len(ds_dict_ocn[f'{var}'][f'{var}'].coords['nlon']))\n",
    "                        ) * np.isnan(ds_dict_ocn[f'{var}'][f'{var}'].isel(time=0))  \n",
    "    mask_array[f'{var}'] = mask_ocean + mask_land\n",
    "    ds_dict_ocn[f'{var}']['TAREA']=ds_dict_ocn[f'{var}']['TAREA'].where(mask_array[f'{var}'] != 1.).isel(time=0)*1e-4 # cm -> m\n",
    "    del mask_array\n",
    "# ds_dict_ocn['SHF']['TAREA']=ds_dict_ocn['SHF']['TAREA'].chunk(chunks=(50,1,67)) \n",
    "ds_dict_ocn['SHF']['TAREA']=ds_dict_ocn['SHF']['TAREA'].compute()\n",
    "ds_dict_ocn['SHF']['TAREA']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['SHF']['TAREA'] = ds_dict_ocn['SHF']['TAREA'].compute()\n",
    "ds_dict_ocn['SHF']['TAREA']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "atm_AREA.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate in the area\n",
    "- We rearrange the size of the chunks and calculate the integral in the data area. We do this for each component, because they have different grids, i.e. different areas of each cell. However, the total area has to be the same. If it is not because of the differences in the grids, we might need to do some interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ocean component\n",
    "fb=(['LWDN_F','SHF','SHF_QSW','LWUP_F','SENH_F'])\n",
    "for var in fb:\n",
    "    ds_dict_ocn[f'{var}'][f'{var}']=ds_dict_ocn[f'{var}'][f'{var}'].chunk(chunks=(50,1980,1,67))\n",
    "    st=f'ds_{var}=[]' \n",
    "    exec(st)\n",
    "    st=f'ds_{var}=ds_dict_ocn[\\'SHF\\'][\\'TAREA\\']*ds_dict_ocn[\\'{var}\\'][\\'{var}\\']*(1e-15)' # PW (1e-15 to convert the units from W to PW) \n",
    "    exec(st)   \n",
    "    st=f'ds_{var}=ds_{var}.sum(dim=[\\'nlat\\',\\'nlon\\'],skipna=True).load()' # PW\n",
    "    exec(st)\n",
    "    print(f'Done with variable: {var}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ocean component does not provide the latent heat flux, but we can calculate it as follows: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Ocean component\n",
    "fb=(['EVAP_F','latent_heat_vapor'])\n",
    "for var in fb:\n",
    "    ds_dict_ocn['EVAP_F'][f'{var}']=ds_dict_ocn['EVAP_F'][f'{var}']\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor'] = ds_dict_ocn['EVAP_F']['latent_heat_vapor'].persist()\n",
    "wait(ds_dict_ocn['EVAP_F']['latent_heat_vapor'])\n",
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_dict_ocn['EVAP_F']['EVAP_F'] = ds_dict_ocn['EVAP_F']['EVAP_F'].persist()\n",
    "wait(ds_dict_ocn['EVAP_F']['EVAP_F'])\n",
    "ds_dict_ocn['EVAP_F']['EVAP_F']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "ds_dict_ocn['SHF']['TAREA'] = ds_dict_ocn['SHF']['TAREA'].persist()\n",
    "wait(ds_dict_ocn['SHF']['TAREA'])\n",
    "ds_dict_ocn['SHF']['TAREA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_LATENT_tmp = ((ds_dict_ocn['SHF']['TAREA' # Area in m2\n",
    "                              ]*ds_dict_ocn['EVAP_F']['EVAP_F' # Mass flux of water vapor in kg/m2/s \n",
    "                                                     ]*ds_dict_ocn['EVAP_F']['latent_heat_vapor' # Latent Heat Vapor in J/kg\n",
    "                                                                            ])).persist()\n",
    "wait(ds_LATENT_tmp)\n",
    "ds_LATENT = ds_LATENT_tmp.sum(dim=['nlat','nlon'],skipna=True)*1e-15 # W -> PW\n",
    "ds_LATENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_LATENT=ds_LATENT.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_dict_ocn['EVAP_F']['latent_heat_vapor'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "ds_LATENT=((ds_dict_ocn['SHF']['TAREA'].isel(                            # Area in m2\n",
    "    member_id=0)*ds_dict_ocn['EVAP_F']['EVAP_F'].isel(                   # Mass flux of water vapor in kg/m2/s \n",
    "    member_id=0)*ds_dict_ocn['EVAP_F']['latent_heat_vapor'].isel(        # Latent Heat Vapor in J/kg\n",
    "    member_id=0)).sum(dim=['nlat','nlon'],skipna=True)*(1e-15)).load()   # PW (1e-15 to convert the units from W to PW) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Ocean component\n",
    "fb=(['LWDN_F','SHF','SHF_QSW','LWUP_F','SENH_F'])\n",
    "for var in fb:\n",
    "    ds_dict_ocn[f'{var}'][f'{var}']=ds_dict_ocn[f'{var}'][f'{var}'].chunk(chunks=(50,1980,1,67))\n",
    "    st=f'ds_{var}=[]' \n",
    "    exec(st)\n",
    "    st=f'ds_{var}=ds_dict_ocn[\\'SHF\\'][\\'TAREA\\']*ds_dict_ocn[\\'{var}\\'][\\'{var}\\']*(1e-15)' # PW (1e-15 to convert the units from W to PW) \n",
    "    exec(st)\n",
    "    st=f'ds_{var}=ds_{var}.sum(dim=[\\'nlat\\',\\'nlon\\'],skipna=True).load()' # PW\n",
    "    exec(st)\n",
    "    print(f'Done with variable: {var}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Atmosphere component\n",
    "fb=(['SHFLX','LHFLX'])\n",
    "for var in fb:\n",
    "    st=f'ds_{var}=[]' \n",
    "    exec(st)\n",
    "    st=f'ds_{var}=atm_AREA*ds_dict_atm[\\'{var}\\'][\\'{var}\\']*(1e-15)' # PW (1e-15 to convert the units from W to PW) \n",
    "    exec(st)\n",
    "    st=f'ds_{var}=ds_{var}.sum(dim=[\\'lat\\',\\'lon\\'],skipna=True).load()' # PW\n",
    "    exec(st)\n",
    "    print(f'Done with variable: {var}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ds_SHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_SHF.mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='SHF1')\n",
    "(ds_LWDN_F+ds_LWUP_F+ds_SHF_QSW+ds_SENH_F+ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='SHF2')\n",
    "#(ds_LWDN_F+ds_LWUP_F).mean(dim=['member_id']).resample(time='5Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='LW')\n",
    "#(ds_SHF_QSW).mean(dim=['member_id']).resample(time='5Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='SW')\n",
    "#(-ds_SHFLX).mean(dim=['member_id']).resample(time='5Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='SH')\n",
    "#(-ds_LHFLX).mean(dim=['member_id']).resample(time='5Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(label='LH')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ticks(ax, ticks, round_to=0.01, center=True):\n",
    "    upperbound = np.ceil(ax.get_ybound()[1]/round_to)\n",
    "    lowerbound = np.floor(ax.get_ybound()[0]/round_to)\n",
    "    dy = upperbound - lowerbound\n",
    "    fit = np.floor(dy/(ticks - 1)) + 1\n",
    "    dy_new = (ticks - 1)*fit\n",
    "    if center:\n",
    "        offset = np.floor((dy_new - dy)/2)\n",
    "        lowerbound = lowerbound - offset\n",
    "    values = np.linspace(lowerbound, lowerbound + dy_new, ticks)\n",
    "    return values*round_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "letts=['A','B','C','D','E','F']\n",
    "vari=['ds_SHF','ds_LWDN_F','ds_LWUP_F','ds_SHF_QSW','ds_SENH_F','ds_LATENT']\n",
    "fig, axs = plt.subplots(1,6, figsize=(25, 7))\n",
    "ds_SHF.mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[0],label='SHF1', linewidth=1,color='blue')\n",
    "(ds_LWDN_F+ds_LWUP_F+ds_SHF_QSW+ds_SENH_F+ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[0],label='SHF2', linewidth=1,color='red')\n",
    "(ds_LWDN_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[1],label='LWDN', linewidth=1,color='maroon')\n",
    "(ds_LWUP_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[2],label='LWUP', linewidth=1,color='green')\n",
    "(ds_SHF_QSW).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[3],label='SW', linewidth=1,color='orange')\n",
    "(ds_SENH_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[4],label='SH', linewidth=1,color='purple')\n",
    "(ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[5],label='LH', linewidth=1,color='c')\n",
    "for i in range(len(axs)):\n",
    "    axs[i].legend(loc=\"upper right\",fontsize=16, ncol=1)\n",
    "    axs[i].grid(color='gray', linestyle='-', linewidth=0.7)\n",
    "    axs[i].set_xlabel('Time [Years]',fontsize=16) \n",
    "    axs[i].tick_params(axis='x', labelsize=16); axs[i].tick_params(axis='y', labelsize=16)\n",
    "    at = AnchoredText(letts[i], prop=dict(size=20), frameon=True, loc='lower left'); at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    axs[i].add_artist(at)\n",
    "    axs[i].set_yticks(calculate_ticks(axs[i], 8))\n",
    "    \n",
    "axs[0].set_ylabel('Heat Flux [PW]',fontsize=16)\n",
    "plt.subplots_adjust(wspace=0.37)\n",
    "plt.savefig('Qnet.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x):\n",
    "    return slope * x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHF_sts=ds_SHF.mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(SHF_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, SHF_sts)\n",
    "mymodel_SHF_sts = list(map(myfunc, x))\n",
    "mymodel_SHF_sts=mymodel_SHF_sts\n",
    "m_SHF_sts=slope*10 # per decade\n",
    "p_SHF_sts=p\n",
    "r_SHF_sts=r*r\n",
    "\n",
    "Neg_sts=(ds_LWDN_F+ds_LWUP_F+ds_SENH_F+ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(Neg_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, Neg_sts)\n",
    "mymodel_Neg_sts = list(map(myfunc, x))\n",
    "mymodel_Neg_sts=mymodel_Neg_sts\n",
    "m_Neg_sts=slope*10 # per decade\n",
    "p_Neg_sts=p\n",
    "r_Neg_sts=r*r\n",
    "             \n",
    "Pos_sts=ds_SHF_QSW.mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31'))\n",
    "x=np.squeeze(range(0,len(Pos_sts)))\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, Pos_sts)\n",
    "mymodel_Pos_sts = list(map(myfunc, x))\n",
    "mymodel_Pos_sts=mymodel_Pos_sts\n",
    "m_Pos_sts=slope*10 # per decade\n",
    "p_Pos_sts=p\n",
    "r_Pos_sts=r*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "letts=['A','B','C','D','E']\n",
    "fig, axs = plt.subplots(1,3, figsize=(20, 7))\n",
    "(ds_LWDN_F+ds_LWUP_F+ds_SHF_QSW+ds_SENH_F+ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[0],label='SHF2', linewidth=1,color='red')\n",
    "(ds_LWDN_F+ds_LWUP_F+ds_SENH_F+ds_LATENT).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[1],label='LWDW+LWUP+SH+LH', linewidth=1,color='maroon')\n",
    "(ds_SHF_QSW).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('1960-01-01','2100-12-31')).plot(\n",
    "    ax=axs[2],label='SW', linewidth=1,color='orange')\n",
    "axs[0].plot((ds_LWDN_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_SHF_sts,color='red',linestyle='dashed')\n",
    "axs[0].annotate(f'{m_SHF_sts*1000:.2f} TW per decade', xy=(0.3, 0.2), color='red',fontsize=20,xycoords=axs[0].transAxes)\n",
    "axs[1].plot((ds_LWDN_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_Neg_sts,color='maroon',linestyle='dashed')\n",
    "axs[1].annotate(f'{m_Neg_sts*1000:.2f} TW per decade', xy=(0.3, 0.2), color='maroon',fontsize=20,xycoords=axs[1].transAxes)\n",
    "axs[2].plot((ds_LWDN_F).mean(dim=['member_id']).resample(time='1Y', closed='left').mean('time').sel(time=slice('2015-01-01','2100-12-31')).coords['time'],mymodel_Pos_sts,color='orange',linestyle='dashed')\n",
    "axs[2].annotate(f'{m_Pos_sts*1000:.2f} TW per decade', xy=(0.3, 0.2), color='orange',fontsize=20,xycoords=axs[2].transAxes)\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].legend(loc=\"upper right\",fontsize=16, ncol=1)\n",
    "    axs[i].grid(color='gray', linestyle='-', linewidth=0.7)\n",
    "    axs[i].set_xlabel('Time [Years]',fontsize=16) \n",
    "    axs[i].tick_params(axis='x', labelsize=16); axs[i].tick_params(axis='y', labelsize=16)\n",
    "    at = AnchoredText(letts[i], prop=dict(size=20), frameon=True, loc='upper left'); at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    axs[i].add_artist(at)\n",
    "    axs[i].set_yticks(calculate_ticks(axs[i], 8))\n",
    "axs[0].set_ylabel('Heat Flux [PW]',fontsize=16)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig('Qnet_1.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2022b",
   "language": "python",
   "name": "npl-2022b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
